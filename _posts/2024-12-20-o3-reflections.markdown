---
layout: post
title:  "o3 reflections"
date:   2024-12-20 18:33
categories: general
---

My initial (mostly negative) reaction:
- I think many of us expected test-time compute to continue scaling, but not for it to happen as quickly or effectively as it did. Seeing the performance on FrontierMath and ARC-AGI was particularly shocking. In my eyes, even if there is nothing *emergently* different about o3-style reasoning, or the gains from verifiable reasoning tasks don't generalize very well, o3 is math/coding AGI. It's certainly better than me, at least.
- If o1/o3 are good enough to kick off recursive self-improvement, or at least be a step change in the rate of AI research progress, then the beginnings of the singularity are here years earlier than I expected. The post-AGI/ASI future feels incredibly murky to me: will it be [benevolent superintelligent](https://darioamodei.com/machines-of-loving-grace) guardian WALL-E world, permanent [technocratic feudalism](https://www.mercatus.org/macro-musings/sam-hammond-ai-techno-feudalism-and-future-state), or actual X-risk AI doom? I hope AI will be good for humanity and I know I should feel excited to see progress, but I don't feel like I even have a sufficiently clear vision of a "good AI future" to know what I'm hoping for.
    - It's particularly hard to feel like I have no grasp for how fast "take-off" might be: 5 years? 20? 100? Not knowing the answer makes a world of difference in how I think I should be responding.
- When o1 was announced in September, I felt mostly fear and apprehension for my career: what should, and could, I be doing? What was my comparative advantage? Was it too late to build a career or have an impact in the fields I cared about? Will all human effort eventually be replaceable? I wasn't sure whether I should be trying to speedrun becoming an AI researcher during my fall quarter; I ended up embracing Berlin instead, which was absolutely the right decision, and these feelings mostly subsided until today.
    - AI will almost certainly be worst for those of us that are new to our careers. After o1, I became convinced that we (i.e. people +/- 2 years of my age) were the last generation for whom AI could be beneficial for our careers, as long as we put a lot of effort into staying upstream and developing domain expertise while we still could. Then once AGI became widely available, we'd be well-positioned to leverage our experience, taste, and network. I was prepared and excited to really work hard at this.
    - Technological change affects every generation, but not at the rates we're seeing today. o3 made me question if my generation was already inside the event horizon; if even our fastest efforts couldn't outpace the march of automation of SWE/MLE careers. Then what?
- Perhaps most stunning was the pace of progress, and the subsequent feeling of being left behind. As [Charlie](https://www.lesswrong.com/posts/PKoGicBvbomuBzJYE/the-nihilism-of-neurips) put it, "Not getting to shape something that's reshaping everything feels like a special kind of meaninglessness". It sucks to feel like I've had basically no contribution and no say in humanity's greatest and last invention. I find myself wishing I had just been born a few years earlier, or that had figured out that AI was *the thing* in high school instead of studying black holes. I don't think it's over for human contributions, but the clock is ticking faster than I ever expected.
    - I think people get drawn into AI research because it feels like the one thing that's *definitely* meaningful; it's humanity's greatest challenge, after all. It feels like progress has become so closed off from the public, and if we're not in the right circles or the right labs, our work is basically meaningless.
    - I'm glad I at least support the development of AGI, but creating it is deeply undemocratic: the world will be changed for everyone, whether we like it or not.
- In general I stave off my nihilism and existential fears by relying on comfortable, familiar patterns and sources of meaning, such as *getting to work on cool problems!* or *feeling like I know how things will work out!*. So I'm not surprised that I had such a visceral, emotional reaction to the news. Even though I've embraced forms of determinism, nihilism, and atheism since ~middle school, I don't think I've really worked through the deeper emotional impacts of truly feeling like there is *nothing out there* to protect me.

How I'm making sense of it:
- My senior year of high school I thought a lot about *what I would do with my life if I only had 20, 10, 5, 2, 1, etc. years left*. That thought experiment helped 18-year-old-me think about what I actually wanted out of life, and convinced her to ditch her complicated 20-year plans (lol). Regardless of your p(doom), I think short timelines has the same effect: these are probably my last productive years, and also my last opportunities to understand the world, and yourself, relatively untouched by technology. How much longer will we have little funny miscommunications, completely private moments, parts of nature not reached by Starlink, or real labors of love? It's such cliche to "live like it's your last day", but we *should* be living and working as if it's our last ~5 years.
- It follows that the marginal value of inauthenticity goes to zero; there is no point in doing things that are not deeply important to you, although I guess [there never really was](https://www.paulgraham.com/love.html). Now is maybe the worst time in history for 10-year-plans, long-timeline delayed gratification, or generally making sacrifices based on some pre-conceived notion of how the world works. No one knows how the world will look in 2 years. Even all the X discourse about ["pivoting to being hot and cool"](https://x.com/apoorvasriniva/status/1870240726170079433) instead of smart is only adding a few years of life support. 
    - My first priority now is figuring out *what I really want*, and going for it on much shorter timelines. I've always been goal-oriented, but stripping away factors like money, status, or anxiety, what actually brings me meaning? Brainstorming here: {wonder, discovery, solving puzzles, contributing to humanity, having fun, understanding myself, understanding the world}. I think I should go all in on *something* while I still have self-determination.
    - My second priority is appreciating the world as we have it: love and mutual understanding, deep companionship, Earth's natural beauty, cultural diversity, inefficient but human systems. I think even in an awesome and abundant AI future I'll still romanticize the "before times". 
- This is hard because I've always been pretty risk-anxious, but I want to take more risks. Actually, one of the first ideas that bubbled up today was dropping out or taking a leave of absence, and going all in on *something*. The future feels so uncertain that taking the "safe path" (quant trading, coterm coursework, PhD research, etc.) can't possibly be the right choice. 
- After years of being intentionally cynical, I'm trying to embrace my imagination again. I realize that the future-thinking people I was skeptical of (sorry, Rationalists) will probably turn out to be right. Things I thought were ridiculous even a year ago -- living forever, the end of capitalism, humanity taking over the galaxy -- have stopped sounding far-fetched. *The future* is so back.