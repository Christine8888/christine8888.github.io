---
layout: post
title:  "Great Power competition, AGI, and global safety"
date:   2024-08-08 19:55
categories: general
---
[INCOMPLETE]

Many of the ideas in this article first formed in the Rhetoric of Generative AI course I took at Stanford in Spring 2024. I'll link my final paper here: [paper]({{ site.baseurl }}/assets/pwr2rba.pdf)

https://x.com/jordanschnyc/status/1796276398069571889?s=46
https://x.com/liron/status/1798896738969014282

There are two scenarios that could be a "get out of jail free" card. The first is that authoritarianism is bad for AI [relevant evidence]. The second is that solving for safety and alignment actually leads to more progress than no-safety no-guardrails acceleration. For example, recent work on sparse auto-encoders and circuit discovery (ex. [this Anthropic paper](https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html)) hints that we can get performance- and safety-enhancing advances from the same family of techniques, killing two birds with one stone.

- When, in the past, have governments nationalized companies?
- The "greater good" is fallacious because of tail risk
