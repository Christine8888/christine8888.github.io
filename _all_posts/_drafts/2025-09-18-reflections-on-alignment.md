---
layout: post
title: Reflections on alignment
date: 2025-9-18 19:10
categories: general
---

I thought it would be a useful exercise to try to reconstruct my mental rationale for working in alignment/safety/robustness/[insert sufficiently broad phrase here] over the past ~year. I have done this reconstruction by a mix of memory, looking at old text messages/notes to self, and . I hope I am a reasonably reliable narrator.

prelude
- In early 2024 I was becoming less interested in doing actual astrophysics, and thinking more about language models. I honestly didn't use language model assistants regularly until then. I had read a [few](https://www.nature.com/articles/s41586-019-1335-8) [papers](https://arxiv.org/abs/2006.11287) freshman year that suggested deep learning might be useful for enabling scientific discovery. I was particularly interested in the pattern of "helping scientists do unsupervised discovery of latent structure", and I thought using natural langauge processing techniques (particularly embeddings??) might prove useful here
- I spent my summer working at first in building semantic search/retrieval products for physics researchers, and then using mechanistic interpretability to understand embeddings[^embeddings].
- I got on X for the first time. In my first ~2 weeks on X, I read Situational Awareness and became extremely worried a bit about humanity's future, superintelligence, geopolitical competition, and generally most predictions Leopold made. The good and bad futures both sounded unimaginable[^fear]
- I started thinking more about frontier language model research, Silicon Valley as an entity, and my stake in the general "state of the world". I also reflected on my intellectual life and vitality, which I felt had decayed in meaningful ways since starting at Stanford


December 2024 -- April? 2025

- Fall 2024 I was abroad in Berlin having a wonderful time, and hence trying not to think about alignment, or AI, or really anything else technical. I was on X occasionally and saw the much-hyped release of . I occasionally listened to Dwarkesh's podcast, particularly his earlier ones.


[^embeddings]: my takeaway is that embeddings might indeed encode latent structure about the fundamental nature of the universe, but this can ~only be surfaced if you know what you're looking for, and there are many more user-friendly ways to generate ontologies